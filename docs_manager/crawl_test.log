[16:56:45] 🚀 Starting COMPREHENSIVE crawl for library 'test' at https://example.com...
[16:56:45] 📋 Configuration:
[16:56:45]    - Target: ALL pages on the domain
[16:56:45]    - Using crawl4ai deep crawling for maximum coverage
[16:56:45]    - BFS strategy for systematic, level-by-level crawling
[16:56:45]    - MAXIMUM coverage with no artificial limits
[16:56:45]    - Progressive indexing enabled
[16:56:45] 
[16:56:45] 🌐 Base domain: example.com
[16:57:01] 🚀 Starting COMPREHENSIVE crawl for library 'test' at https://example.com...
[16:57:01] 📋 Configuration:
[16:57:01]    - Target: ALL pages on the domain
[16:57:01]    - Using crawl4ai deep crawling for maximum coverage
[16:57:01]    - BFS strategy for systematic, level-by-level crawling
[16:57:01]    - MAXIMUM coverage with no artificial limits
[16:57:01]    - Progressive indexing enabled
[16:57:01] 
[16:57:01] 🌐 Base domain: example.com
[16:57:13] 🚀 Starting COMPREHENSIVE crawl for library 'test' at https://example.com...
[16:57:13] 📋 Configuration:
[16:57:13]    - Target: ALL pages on the domain
[16:57:13]    - Using crawl4ai deep crawling for maximum coverage
[16:57:13]    - BFS strategy for systematic, level-by-level crawling
[16:57:13]    - MAXIMUM coverage with no artificial limits
[16:57:13]    - Progressive indexing enabled
[16:57:13] 
[16:57:13] 🌐 Base domain: example.com
[16:57:13] 🕷️ Starting MAXIMUM coverage crawl4ai deep crawl...
[16:57:13]    📊 Max depth: 20
[16:57:13]    📊 Max pages: 10000
[16:57:13]    📊 Word threshold: 0
[16:57:13] 
[16:57:14] 🔍 Starting crawl with MAXIMUM coverage settings...
[16:57:15] 🔄 [1] CRAWLING: https://example.com
[16:57:15]    📊 Depth: 0
[16:57:15]    ✅ Success: True
[16:57:15]    📝 Has markdown: True
[16:57:15]    📏 Markdown length: 230
[16:57:15]    🔗 Discovered 2 links
[16:57:15] ❌ Crawl iteration error: slice(None, 3, None)
[16:57:15] 📊 COMPREHENSIVE Crawling Summary:
[16:57:15]    ✅ Successfully processed and indexed: 0 pages
[16:57:15]    🔗 Total URLs visited: 1
[16:57:15]    ❌ Failed to process: 0 pages
[16:57:15]    📦 Total chunks created and indexed: 0
[16:57:15] 
[17:00:06] 🚀 Starting COMPREHENSIVE crawl for library 'test' at https://example.com...
[17:00:06] 📋 Configuration:
[17:00:06]    - Target: ALL pages on the domain
[17:00:06]    - Using crawl4ai deep crawling for maximum coverage
[17:00:06]    - BFS strategy for systematic, level-by-level crawling
[17:00:06]    - MAXIMUM coverage with no artificial limits
[17:00:06]    - Progressive indexing enabled
[17:00:06] 
[17:00:06] 🌐 Base domain: example.com
[17:00:06] 🕷️ Starting MAXIMUM coverage crawl4ai deep crawl...
[17:00:06]    📊 Max depth: 20
[17:00:06]    📊 Max pages: 10000
[17:00:06]    📊 Word threshold: 0
[17:00:06] 
[17:00:06] 🔍 Starting crawl with MAXIMUM coverage settings...
[17:00:08] 🔄 [1] CRAWLING: https://example.com
[17:00:08]    📊 Depth: 0
[17:00:08]    ✅ Success: True
[17:00:08]    📝 Has markdown: True
[17:00:08]    📏 Markdown length: 230
[17:00:08]    🔗 Discovered 2 links
[17:00:08]    🔗 Link discovery error: slice(None, 3, None)
[17:00:08]    📄 Preview: # Example Domain This domain is for use in illustrative examples in documents. You may use this doma...
[17:00:08]    📊 Content analysis:
[17:00:08]       📏 Total lines: 4
[17:00:08]       📏 Non-empty lines: 3
[17:00:08]       🎯 Headers found: 1
[17:00:08]       📍 Header levels: [1]
[17:00:08]    ✅ Content length: 230 characters
[17:00:08]    📝 Title: Example.Com
[17:00:08]    🎯 Processing document for indexing...
[17:00:08]    🔄 Splitting document into chunks...
[17:00:08]    📦 Created 1 chunks.
[17:00:08]    📤 Adding 1 chunks to Pinecone...
[17:00:13]    ✅ Successfully added chunks to Pinecone.
[17:00:13]    🎉 Page 1 fully processed and indexed!
[17:00:13] 
[17:00:13] 📊 COMPREHENSIVE Crawling Summary:
[17:00:13]    ✅ Successfully processed and indexed: 1 pages
[17:00:13]    🔗 Total URLs visited: 1
[17:00:13]    ❌ Failed to process: 0 pages
[17:00:13]    📦 Total chunks created and indexed: 1
[17:00:13] 
[17:03:16] 🚀 Starting COMPREHENSIVE crawl for library 'test' at https://example.com...
[17:03:16] 📋 Configuration:
[17:03:16]    - Target: ALL pages on the domain
[17:03:16]    - Using crawl4ai deep crawling for maximum coverage
[17:03:16]    - BFS strategy for systematic, level-by-level crawling
[17:03:16]    - MAXIMUM coverage with no artificial limits
[17:03:16]    - Progressive indexing enabled
[17:03:16] 
[17:03:16] 🌐 Base domain: example.com
[17:03:16] 🕷️ Starting MAXIMUM coverage crawl4ai deep crawl...
[17:03:16]    📊 Max depth: 20
[17:03:16]    📊 Max pages: 10000
[17:03:16]    📊 Word threshold: 0
[17:03:16] 
[17:03:17] 🔍 Starting crawl with MAXIMUM coverage settings...
[17:03:18] 🔄 [1] CRAWLING: https://example.com
[17:03:18]    📊 Depth: 0
[17:03:18]    ✅ Success: True
[17:03:18]    📝 Has markdown: True
[17:03:18]    📏 Markdown length: 230
[17:03:18]    🔗 Discovered 2 links
[17:03:18]    🔗 Link discovery error: slice(None, 3, None)
[17:03:18]    📄 Preview: # Example Domain This domain is for use in illustrative examples in documents. You may use this doma...
[17:03:18]    📊 Content analysis:
[17:03:18]       📏 Total lines: 4
[17:03:18]       📏 Non-empty lines: 3
[17:03:18]       🎯 Headers found: 1
[17:03:18]       📍 Header levels: [1]
[17:03:18]       📋 Headers:
[17:03:18]          1. # Example Domain
[17:03:18]    ✅ Content length: 230 characters
[17:03:18]    📝 Title: Example.Com
[17:03:18]    🎯 Processing document for indexing...
[17:03:18]    🔄 Splitting document into chunks...
[17:03:18]    📦 Created 1 chunks.
[17:03:18]    📤 Adding 1 chunks to Pinecone...
[17:03:19]    ✅ Successfully added chunks to Pinecone.
[17:03:19]    🎉 Page 1 fully processed and indexed!
[17:03:19] 
[17:03:19] 📊 COMPREHENSIVE Crawling Summary:
[17:03:19]    ✅ Successfully processed and indexed: 1 pages
[17:03:19]    🔗 Total URLs visited: 1
[17:03:19]    ❌ Failed to process: 0 pages
[17:03:19]    📦 Total chunks created and indexed: 1
[17:03:19] 
