[16:56:45] ğŸš€ Starting COMPREHENSIVE crawl for library 'test' at https://example.com...
[16:56:45] ğŸ“‹ Configuration:
[16:56:45]    - Target: ALL pages on the domain
[16:56:45]    - Using crawl4ai deep crawling for maximum coverage
[16:56:45]    - BFS strategy for systematic, level-by-level crawling
[16:56:45]    - MAXIMUM coverage with no artificial limits
[16:56:45]    - Progressive indexing enabled
[16:56:45] 
[16:56:45] ğŸŒ Base domain: example.com
[16:57:01] ğŸš€ Starting COMPREHENSIVE crawl for library 'test' at https://example.com...
[16:57:01] ğŸ“‹ Configuration:
[16:57:01]    - Target: ALL pages on the domain
[16:57:01]    - Using crawl4ai deep crawling for maximum coverage
[16:57:01]    - BFS strategy for systematic, level-by-level crawling
[16:57:01]    - MAXIMUM coverage with no artificial limits
[16:57:01]    - Progressive indexing enabled
[16:57:01] 
[16:57:01] ğŸŒ Base domain: example.com
[16:57:13] ğŸš€ Starting COMPREHENSIVE crawl for library 'test' at https://example.com...
[16:57:13] ğŸ“‹ Configuration:
[16:57:13]    - Target: ALL pages on the domain
[16:57:13]    - Using crawl4ai deep crawling for maximum coverage
[16:57:13]    - BFS strategy for systematic, level-by-level crawling
[16:57:13]    - MAXIMUM coverage with no artificial limits
[16:57:13]    - Progressive indexing enabled
[16:57:13] 
[16:57:13] ğŸŒ Base domain: example.com
[16:57:13] ğŸ•·ï¸ Starting MAXIMUM coverage crawl4ai deep crawl...
[16:57:13]    ğŸ“Š Max depth: 20
[16:57:13]    ğŸ“Š Max pages: 10000
[16:57:13]    ğŸ“Š Word threshold: 0
[16:57:13] 
[16:57:14] ğŸ” Starting crawl with MAXIMUM coverage settings...
[16:57:15] ğŸ”„ [1] CRAWLING: https://example.com
[16:57:15]    ğŸ“Š Depth: 0
[16:57:15]    âœ… Success: True
[16:57:15]    ğŸ“ Has markdown: True
[16:57:15]    ğŸ“ Markdown length: 230
[16:57:15]    ğŸ”— Discovered 2 links
[16:57:15] âŒ Crawl iteration error: slice(None, 3, None)
[16:57:15] ğŸ“Š COMPREHENSIVE Crawling Summary:
[16:57:15]    âœ… Successfully processed and indexed: 0 pages
[16:57:15]    ğŸ”— Total URLs visited: 1
[16:57:15]    âŒ Failed to process: 0 pages
[16:57:15]    ğŸ“¦ Total chunks created and indexed: 0
[16:57:15] 
[17:00:06] ğŸš€ Starting COMPREHENSIVE crawl for library 'test' at https://example.com...
[17:00:06] ğŸ“‹ Configuration:
[17:00:06]    - Target: ALL pages on the domain
[17:00:06]    - Using crawl4ai deep crawling for maximum coverage
[17:00:06]    - BFS strategy for systematic, level-by-level crawling
[17:00:06]    - MAXIMUM coverage with no artificial limits
[17:00:06]    - Progressive indexing enabled
[17:00:06] 
[17:00:06] ğŸŒ Base domain: example.com
[17:00:06] ğŸ•·ï¸ Starting MAXIMUM coverage crawl4ai deep crawl...
[17:00:06]    ğŸ“Š Max depth: 20
[17:00:06]    ğŸ“Š Max pages: 10000
[17:00:06]    ğŸ“Š Word threshold: 0
[17:00:06] 
[17:00:06] ğŸ” Starting crawl with MAXIMUM coverage settings...
[17:00:08] ğŸ”„ [1] CRAWLING: https://example.com
[17:00:08]    ğŸ“Š Depth: 0
[17:00:08]    âœ… Success: True
[17:00:08]    ğŸ“ Has markdown: True
[17:00:08]    ğŸ“ Markdown length: 230
[17:00:08]    ğŸ”— Discovered 2 links
[17:00:08]    ğŸ”— Link discovery error: slice(None, 3, None)
[17:00:08]    ğŸ“„ Preview: # Example Domain This domain is for use in illustrative examples in documents. You may use this doma...
[17:00:08]    ğŸ“Š Content analysis:
[17:00:08]       ğŸ“ Total lines: 4
[17:00:08]       ğŸ“ Non-empty lines: 3
[17:00:08]       ğŸ¯ Headers found: 1
[17:00:08]       ğŸ“ Header levels: [1]
[17:00:08]    âœ… Content length: 230 characters
[17:00:08]    ğŸ“ Title: Example.Com
[17:00:08]    ğŸ¯ Processing document for indexing...
[17:00:08]    ğŸ”„ Splitting document into chunks...
[17:00:08]    ğŸ“¦ Created 1 chunks.
[17:00:08]    ğŸ“¤ Adding 1 chunks to Pinecone...
[17:00:13]    âœ… Successfully added chunks to Pinecone.
[17:00:13]    ğŸ‰ Page 1 fully processed and indexed!
[17:00:13] 
[17:00:13] ğŸ“Š COMPREHENSIVE Crawling Summary:
[17:00:13]    âœ… Successfully processed and indexed: 1 pages
[17:00:13]    ğŸ”— Total URLs visited: 1
[17:00:13]    âŒ Failed to process: 0 pages
[17:00:13]    ğŸ“¦ Total chunks created and indexed: 1
[17:00:13] 
[17:03:16] ğŸš€ Starting COMPREHENSIVE crawl for library 'test' at https://example.com...
[17:03:16] ğŸ“‹ Configuration:
[17:03:16]    - Target: ALL pages on the domain
[17:03:16]    - Using crawl4ai deep crawling for maximum coverage
[17:03:16]    - BFS strategy for systematic, level-by-level crawling
[17:03:16]    - MAXIMUM coverage with no artificial limits
[17:03:16]    - Progressive indexing enabled
[17:03:16] 
[17:03:16] ğŸŒ Base domain: example.com
[17:03:16] ğŸ•·ï¸ Starting MAXIMUM coverage crawl4ai deep crawl...
[17:03:16]    ğŸ“Š Max depth: 20
[17:03:16]    ğŸ“Š Max pages: 10000
[17:03:16]    ğŸ“Š Word threshold: 0
[17:03:16] 
[17:03:17] ğŸ” Starting crawl with MAXIMUM coverage settings...
[17:03:18] ğŸ”„ [1] CRAWLING: https://example.com
[17:03:18]    ğŸ“Š Depth: 0
[17:03:18]    âœ… Success: True
[17:03:18]    ğŸ“ Has markdown: True
[17:03:18]    ğŸ“ Markdown length: 230
[17:03:18]    ğŸ”— Discovered 2 links
[17:03:18]    ğŸ”— Link discovery error: slice(None, 3, None)
[17:03:18]    ğŸ“„ Preview: # Example Domain This domain is for use in illustrative examples in documents. You may use this doma...
[17:03:18]    ğŸ“Š Content analysis:
[17:03:18]       ğŸ“ Total lines: 4
[17:03:18]       ğŸ“ Non-empty lines: 3
[17:03:18]       ğŸ¯ Headers found: 1
[17:03:18]       ğŸ“ Header levels: [1]
[17:03:18]       ğŸ“‹ Headers:
[17:03:18]          1. # Example Domain
[17:03:18]    âœ… Content length: 230 characters
[17:03:18]    ğŸ“ Title: Example.Com
[17:03:18]    ğŸ¯ Processing document for indexing...
[17:03:18]    ğŸ”„ Splitting document into chunks...
[17:03:18]    ğŸ“¦ Created 1 chunks.
[17:03:18]    ğŸ“¤ Adding 1 chunks to Pinecone...
[17:03:19]    âœ… Successfully added chunks to Pinecone.
[17:03:19]    ğŸ‰ Page 1 fully processed and indexed!
[17:03:19] 
[17:03:19] ğŸ“Š COMPREHENSIVE Crawling Summary:
[17:03:19]    âœ… Successfully processed and indexed: 1 pages
[17:03:19]    ğŸ”— Total URLs visited: 1
[17:03:19]    âŒ Failed to process: 0 pages
[17:03:19]    ğŸ“¦ Total chunks created and indexed: 1
[17:03:19] 
